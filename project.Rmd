---
title: "Analysis of finance-related Reddit Communities"
output: rmdformats::readthedown
bibliography: references.bib
csl: ieee.csl

---

# Overview and Motivation

In January of 2021, a major short squeeze of the stock of video game retailer GameStop and other securities took place[@RePEc]. A major driving force behind this event was the subreddit wallstreetbets, where users discussed the market situation around GameStop, focussing on the amount of short positions major hedge funds held for this stock and a “David versus Goliath”-situation unfolded. What followed was a large influx of users to the subreddit, who were convinced the stock had to go up and hedge funds had to pay for it. 
 
Several interesting questions regarding the influence of these subreddits on the stock market emerge from this event. In this project, we want to detect communities in finance related subreddits and mine their sentiment towards discussed stocks to measure if they either have significant influence on or can predict market movements.

# Related Work 

Prediction of market movements in relation to stock prices and volume of traded stocks has been widely studied mainly because it can be used as a tool for  investing in profitable stocks with small risks. The prediction of stock prices has been carried out successfully with different machine learning  techniques [@8212715].

In terms of community detection, it is possible to identify communities inside social media platforms. These communities can be roughly defined as a subset of entities that have elements in common, which could be, for example, a topic. In a broader sense, the communities are represented as subgraphs  which can be identified via  community detection methods such as graph partitioning and graph clustering. On [@papadopoulos2012community], a survey of different methods to identify communities in the context of social media.

Lastly, finding correlations between social media interaction and stock market is a complicated task. In this area, [@ruiz2012correlating] presents an exploration between twitter messages and stock-market events related to a group of specific companies. As result, it was possible to find correlations between some features from the messages and the traded volume.

# Initial Questions

Based on the objectives on our proposal, the initial questions that we tried to solve were divided in three aspects of our project, which are data extraction and storage, community detection, sentiment analysis, and correlation between sentiment and stock market information. The questions are as follows: 
 
1. How can we create communities inside subreddits and extract the stocks discussed inside each community?
2. Is it possible to correlate the sentiment towards the stocks discussed in each community to  the stock market related data?

The questions evolved during the first stages of data collecting and research about algorithms for clustering and sentiment analysis. The questions change into more specific and new questions were included. The final questions for the project are:

### 1. How can we identify subcommunities inside subreddit and which measures can be used to evaluate the subcommunities?
A subreddit is already a big community inside Reddit, which allows the interaction between users. It is important to represent the interaction from the users in order to find relations and smaller clusters from users which we can analyze afterwards.  

### 2. Is it possible to identify stocks that are discussed in each subcommunity and the sentiment related to the stocks in a specific point of time?

### 3. Based on the stocks and sentiments towards them inside the communities, is it possible to find a correlation with the stock prices?

# Data

*source, scraping method, cleanup, storage, etc.*

For this project we have two main sources of data, namely Reddit and Yahoo Finance. 

## Reddit comments
The data that we use from the subreddits are comments on posts and the post content. For this purpose we used the library RedditExtractoR and extract daily the comments on the posts with more than 20 comments. We got the data from 11.01.2020 for the subreddits "Wallstreetbets" and "stocks". 

The structure of the retrieved comments is presented in the following table:

```{r include=FALSE}
library(kableExtra)
library(tidyverse)
library(ggplot2)
library(plotly)
library(lubridate)
```


```{r raw_data, echo=FALSE,warning = FALSE}
source("DB_connection.R")

databaseName <- "reddit"
collectionName <- "stocks"
initial_time <-"2020-12-23"
end_time <-"2020-12-27"

raw_data <- loadDataDates(databaseName,collectionName,initial_time,end_time)

kbl(head(raw_data,5)) 
```
Each comment contains the information of the content of the comment, user that make the comment, the related post with content and author. Moreover, the column "structure" define the position occupied by the comment in the hierarchy of comments related to a post.

The problem that we identify on the data was that once a user from reddit is deleted, the comment will have in the column author or user the value "[deleted]". With this situation is not possible to identify correctly the users and create the right connection between them, which is really important for the community detection. To visualize this problem. The following graphics, shows the amount of comments per day and the amount of comments with author or user with value "[deleted]".

```{r proportionDeletedUsers, echo=FALSE,warning = FALSE}
databaseName <- "reddit"
collectionName <- "stocks"
#get all the interactions with 
deleted_comments <-getPostDeletedUser(databaseName,collectionName)
colnames(deleted_comments) <- c('dates','deleted_users')
deletedUsers_Authors <-getDeletedPost(databaseName,collectionName)
colnames(deletedUsers_Authors) <- c('dates','deleted_auth_users')
post_per_day <-getPostPerDay(databaseName,collectionName)
colnames(post_per_day) <- c('dates','total_comments')

merged_data <- merge(x = post_per_day, y = deleted_comments, 
                    by = "dates", all = TRUE) %>%
  merge(.,deletedUsers_Authors, by="dates",all=TRUE) 
              
merged_data[is.na(merged_data)] <- 0
merged_data$percentageLost <- merged_data$deleted_users / merged_data$total_comments
merged_data$percentageAuthorUsers <- merged_data$deleted_auth_users / merged_data$total_comments
merged_data$dates <- ymd(merged_data$dates)

ggplotly(ggplot(data=merged_data, aes(x=dates)) +
           geom_line(aes(y=percentageLost), color="darkred")+
           geom_line(aes(y=percentageAuthorUsers), color="steelblue")+ 
           scale_x_date(date_labels = "%m-%Y"))
```

To work around this problem. We deleted all of the comments made by a user that was deleted and renamed the authors based on the post.

## Stock data

## Feature engineering

### Community detection

For the community detection we used the library igraph. The subreddit information was transformed into a graph representation. The nodes represent the users from the subreddit and the edges represent the interaction between the users. The interactions are classified into:

**1. Answers to a post:** direct comments to a post. For this case an edge is created between the user and the author of the post

**2. Answer to a comment:** comments made as answer to other comments. For this case an edge is created between two users and it requires to use the hierarchy of comments to find the proper connection.



**Clustering Algorithms: **

For the community detection we considered the algorithms Louvain, Infomap and label propagation. To evaluate the communities generated by the algorithms, we use the measure modularity

# Exploratory Data Analysis

*What visualizations did you use to look at your data in different ways?*
*What are the different machine learning methods you considered?*
*Justify the decisions you made, and show any major changes to your ideas.*
*How did you reach these conclusions?*

# Final Analysis

*What did you learn about the data?*
*How did you answer the questions?*
*How can you justify your answers?*

# References
