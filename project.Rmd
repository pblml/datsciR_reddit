---
title: "Analysis of finance-related Reddit Communities"
output: rmdformats::readthedown
bibliography: references.bib
csl: ieee.csl

---
```{r setup, include=FALSE}
library(kableExtra)
library(tidyverse)
library(ggplot2)
library(plotly)
library(lubridate)
library(igraph)
library(visNetwork)
library(BatchGetSymbols)
library(scales)
library(sentimentr)
source("DB_connection.R")
set.seed(123)
```

# Overview and Motivation

In January of 2021, a major short squeeze of the stock of video game retailer GameStop and other securities took place[@RePEc]. A major driving force behind this event was the subreddit wallstreetbets, where users discussed the market situation around GameStop, focussing on the amount of short positions major hedge funds held for this stock and a “David versus Goliath”-situation unfolded. What followed was a large influx of users to the subreddit, who were convinced the stock had to go up and hedge funds had to pay for it. 
 
Several interesting questions regarding the influence of these subreddits on the stock market emerge from this event. In this project, we want to detect communities in finance related subreddits and mine their sentiment towards discussed stocks to measure if they either have significant influence on or can predict market movements.

# Related Work 

Prediction of market movements in relation to stock prices and volume of traded stocks has been widely studied mainly because it can be used as a tool for  investing in profitable stocks with small risks. The prediction of stock prices has been carried out successfully with different machine learning  techniques [@8212715].

In terms of community detection, it is possible to identify communities inside social media platforms. These communities can be roughly defined as a subset of entities that have elements in common, which could be, for example, a topic. In a broader sense, the communities are represented as subgraphs  which can be identified via  community detection methods such as graph partitioning and graph clustering. On [@papadopoulos2012community], a survey of different methods to identify communities in the context of social media.

Lastly, finding correlations between social media interaction and stock market is a complicated task. In this area, [@ruiz2012correlating] presents an exploration between twitter messages and stock-market events related to a group of specific companies. As result, it was possible to find correlations between some features from the messages and the traded volume.

# Initial Questions

Based on the objectives on our proposal, the initial questions that we tried to solve were divided in three aspects of our project, which are data extraction and storage, community detection, sentiment analysis, and correlation between sentiment and stock market information. The questions are as follows: 
 
1. How can we create communities inside subreddits and extract the stocks discussed inside each community?
2. Is it possible to correlate the sentiment towards the stocks discussed in each community to  the stock market related data?

The questions evolved during the first stages of data collecting and research about algorithms for clustering and sentiment analysis. The questions change into more specific and new questions were included. The final questions for the project are:

### 1. How can we identify subcommunities inside subreddit and which measures can be used to evaluate the subcommunities?
A subreddit is already a big community inside Reddit, which allows the interaction between users. It is important to represent the interaction from the users in order to find relations and smaller clusters from users which we can analyze afterwards.  

### 2. How can we identify stocks that are discussed in each subcommunity and the sentiment related to the stocks?
For this question we would like to find strategies that allows us to identify the stocks that are mentioned inside each community and the sentiment (positive, negative or neutral) towards the stocks.

### 3. Based on the stocks and sentiments towards them inside the communities, is it possible to find a correlation with the stock prices?
Considering the Gamestop event we would like to find out if there are relations between the sentiment that the users express in the subreddit towards a stock with the real stock prices. 

# Data


For this project we have two main sources of data, namely Reddit and Yahoo Finance. 

## Reddit comments
The data that we use from the subreddits are comments on posts and the post content. For this purpose we used the library RedditExtractoR and extract daily the comments by selecting the top 5 threads posted on the specific date and the first 100 comments per thread. We got the data from 11.01.2020 to 01.05.2021 for the subreddits "Wallstreetbets" and "stocks". 

**TODO: Pushshift is used to fetch the relevant urls, a modified version of RedditExtractoR::get_reddit is used to fetch the data**  

The structure of the retrieved comments is presented in the following table:


```{r raw_data, echo=FALSE,warning = FALSE}

databaseName <- "reddit"
collectionName <- "stocks"
initial_time <-"2021-02-10"
end_time <-"2021-02-12"

sample <- loadDataDates(databaseName,collectionName,initial_time,end_time) %>%
  subset(select = -c(ticker,sentiment))

kbl(head(sample,3)) %>%
  kable_material(c("hover")) %>%
  kable_styling(full_width = FALSE)
```
Each comment contains the information of the content of the comment, user that make the comment, the related post including the content and author. Moreover, the column "structure" define the position occupied by the comment in the hierarchy of comments related to a post.

One problem that we identify on the data was that once a user from reddit is deleted, the comment will have in the column author or user the value "[deleted]". With this situation is not possible to correctly identify the users and create the right connection between them, which is really important for the community detection. The following graphics present a visualization of the proportion of comments per day that have deleted author or user. 

```{r proportionDeletedUsers, echo=FALSE,warning = FALSE}
collectionName <- "stocks"
source("reddit_related_functions.R")

#get proportion of missing comments due to deleted users
stock_comments <- get_info_deleted_users("stocks")
wallstreetbets_comments <-get_info_deleted_users("wallstreetbets")


ggplotly(ggplot(data=stock_comments, aes(x=dates)) +
           geom_line(aes(y=percentage_comments_del_users, colour="Users"))+
           geom_line(aes(y=percentage_comments_del_authors, colour="Authors"))+ 
           scale_color_manual(name = "Deleted  object",values = c(
             'Users' = 'darkred',
             'Authors' = 'steelblue'))+
           scale_x_date(date_labels = "%m-%Y")+
           labs(title="Proportion of comments with deleted author/user subreddit Stocks",
                x = "Comment Date",
                y = "Proportion of comments with deleted user/author"))

ggplotly(ggplot(data=wallstreetbets_comments, aes(x=dates)) +
           geom_line(aes(y=percentage_comments_del_users, colour="Users"))+
           geom_line(aes(y=percentage_comments_del_authors, colour="Authors"))+ 
           scale_color_manual(name = "Deleted  object",values = c(
             'Users' = 'darkred',
             'Authors' = 'steelblue'))+ 
           scale_x_date(date_labels = "%m-%Y")+
           labs(title="Proportion of comments with deleted author/user subreddit Wallstreetbets",
                x = "Comment Date",
                y = "Proportion of comments with deleted user/author"))
```

If we don't consider comments with deleted user or author we are loosing too many comments, which is also not ideal. With the graphic we can observe that the biggest problem is when authors are missing, for those cases we can lose more than 50% of the comments for a given date. To work around this problem. We decided to delete all of the comments made by a user that was deleted and renamed the authors based on the post. 

Moreover, We stored the subreddit data in a cloud database service [MongoDB Atlas](https://www.mongodb.com/es/cloud/atlas). In this way we can reproduce our results from previous data even if the users change overtime in Reddit.

The data is stored in a single database name "reddit", which has two separate collections named "stocks" and "wallstreet" with the raw data. The manipulation of the data was made by using the library [mongolite](https://cran.r-project.org/web/packages/mongolite/mongolite.pdf).

## Stock data

To get the market data for the relevant tickers, the package ['quantmod'](https://www.quantmod.com/) is used.
It takes a list of ticker symbols, a start and an end date and returns the market
data in a clean format.

```{r stockDataExample }
tickers <- c("GME","TSLA","AMC")
l.out <- BatchGetSymbols(tickers = tickers, 
                         first.date = lubridate::ymd_hms("2021-06-07 00:00:00"),
                         last.date = lubridate::ymd_hms("2021-06-10 00:00:00"), 
                         freq.data = "daily",
                         cache.folder = file.path(tempdir(), 
                                                  'BGS_Cache'))
kbl(l.out$df.tickers)
```


Due to its caching functionality, prices have to be fetched only once, subsequent 
calls first check if the cached file contains all the relevant data.


## Feature engineering

### Sentiment analysis

To examine the correlation between market data and sentiment, this measure has to be extracted from every comment.

* For calculation of the sentiment the package  [sentimentr](https://cran.r-project.org/web/packages/sentimentr/sentimentr.pdf)
* It uses a dictionary based approach and also takes modifiers and negators into account, which is a major advantage in comparison to the package [SentimentAnalysis](https://cran.r-project.org/web/packages/SentimentAnalysis/SentimentAnalysis.pdf) (finance specific dict!)

To consider the financial jargon used in these subreddits, the Loughran-McDonald dictionary is used for assigning polarity values to the words of a comment. The dictionary also has to be modified for the special slang of [r/wallstreetbets](www.reddit.com/r/wallstreetbets)

```{r}
wsb_specific_terms <- data.frame(x=c("to the moon", "cant go tits up", "tendies", "diamond hands", "paper hands"), y=c(1,1,1,1,-1)) %>% as_key()

modified_dict <- lexicon::hash_sentiment_loughran_mcdonald %>% 
  update_polarity_table(x=wsb_specific_terms)

print(modified_dict %>% filter(y>0) %>% sample_n(10))
print(modified_dict %>% filter(y<0) %>% sample_n(10))
```

```{r}
wsb <- loadData("reddit", "wallstreetbets")
stocks <- loadData("reddit", "stocks")
```

After adding the sentiment, we take a look at the the most positive and negative comments.

```{r}
print(stocks %>% arrange(sentiment) %>% head() %>% select(comment, sentiment))
print(stocks %>% arrange(desc(sentiment)) %>% head() %>% select(comment, sentiment))

print(wsb %>% arrange(sentiment) %>% head() %>% select(comment, sentiment))
print(wsb %>% arrange(desc(sentiment)) %>% head() %>% select(comment, sentiment))
```


### Ticker extraction

To find the tickers each comment has mentioned a regular expression is used to identify stock symbols.

```{r}
stringr::str_match_all("I bought AMC and BB", '[^A-Za-z0-9]+([A-Z]{2,4})[^A-Za-z0-9]*')[[1]][,2]
```

This regex extracts every two to four uppercase letter long sequences if it is lead and followed by a non-alphanumeric character.

Disadvantages of this method in comparison to a fixed dictionary are:
  * False positives like "WSB" for Wallstreetbets or "ETF" for Exchange-traded fund. A remedy to this is an expansion of the regex or a subsequent filter on the most mentioned false positives.
  * Actual company names are not recognized, so if GameStop gets mentioned, it is not detected as GME.

### Community detection

For the community detection we used the library igraph. The subreddit's information was transformed into a graph representation. Nodes represent the users of the subreddit, while interaction between the users is represented by edges. The interactions are classified into:

**1. Answers to a post:** direct comments to a post. For this case an edge is created between the user and the author of the post.

**2. Answer to a comment:** comments made as answer to other comments. For this case an edge is created between two users and it requires to use the hierarchy of comments to find the proper connection.

The weight of the edges is given by the number of interactions between the two users. A visual representation of the graph created by the comments between the dates 2020-11-03 and 2020-11-06 for the subreddit stocks is as follows: 

```{r graphcreation, echo=FALSE ,warning = FALSE}

initial_time <-"2021-01-23"
end_time <-"2021-01-27"
collectionName <- "stocks"

raw_data <- stocks %>%
  filter(user!="[deleted]",comm_date>=lubridate::ymd(initial_time),
         comm_date<=lubridate::ymd(end_time))

#for authors that have name [deleted] a new name is assigned with the structure "author_number"
new_authors_names <-raw_data %>%
  filter(author=="[deleted]") %>%
  distinct(link) %>%
  mutate(author_name = paste0("author_",row_number(.)))

raw_data <-raw_data %>%
  left_join(., new_authors_names, by=c( "link"="link")) %>%
  mutate(author = ifelse(author == "[deleted]", author_name, author))

#create dataframe with information about direct comments to posts and nested comments
comments_posts <- raw_data %>%
  filter(!grepl("_",structure)) %>%
  rename(
    from = user,
    to = author
  )%>%
  subset( select = c(from,to))

nested_comments <-raw_data %>% 
  mutate(
    from = structure,
    to = gsub("^(.*)_\\d+$", "\\1", structure) 
  )%>%
  subset( select = c(user,from,to,link))%>%
  left_join(., ., by=c( "link"="link","from"="to")) %>% 
  drop_na("user.y") %>%
  select("from"=user.y, "to"=user.x) 

#combination of the connections between reddit users
connections <-rbind(comments_posts, nested_comments) %>% 
  filter(from!=to) %>% 
  group_by(from, to) %>%
  summarise(weight = n()) %>% 
  ungroup() %>%
  mutate(width = weight+5)

#create igraph object
g <- graph_from_data_frame(connections,directed=FALSE)

plot(g,vertex.label=NA, vertex.color="blue", vertex.size=5)
```


**Clustering Algorithms: **

For the community detection we considered the algorithms [Louvain](https://en.wikipedia.org/wiki/Louvain_method), [Infomap](https://www.mapequation.org/infomap/) and [label propagation](https://en.wikipedia.org/wiki/Label_propagation_algorithm). Since there is no external information about the communities detected, modularity is used to evaluate the quality of the clustering. Modularity measures how strong is the community structure generated and it can be defined as "a normalized tradeoff between edges covered by clusters and squared cluster degree sums"[@modularity]
s
In order to compare the cluster algorithms, we run all of them 4 different samples of the data as shown in the following table. We used both subreddits wallstreetbets and stocks and have different sizes on samples. With this test we recorded the modularity, execution time and number of clusters created. The results from the evaluation are given in the following plots. 

```{r include=FALSE}
source("cluster_evaluation.R")
```


```{r cluster_evaluation, echo=FALSE, warning=FALSE}
kbl(evaluations,
    col.names = c("Test",
                  "Collection Name",
                  "Initial Date",
                  "Final Date",
                  "Sample size"),
    caption = "Test samples")

modularity_clusters <- result %>%
  filter(measure == "modularity") 

ggplotly(ggplot(data=modularity_clusters, aes(x=id_test))+
           geom_bar(aes(y=value,fill=algorithm), 
                    stat = "identity", position = "dodge")+
           labs( x = "Test",
                 y = "Modularity",
                 title = "Modularity per algorithm for each test"))

```


In terms of modularity, Louvain performed better than Infomap and Label Propagation. The highest possible value for modularity is 1.0 when all of the clusters are disconnected subgraphs. The values that Louvain achieved are higher than 0.55 which represents a strong community structure. 


```{r number_clusters, echo=FALSE, warning=FALSE}

cluster_numbers <- result %>%
  filter(measure == "clusters")

ggplotly(ggplot(data=cluster_numbers, aes(x=id_test,y=value,fill=algorithm))+
           geom_bar(aes(label=clusters_one_user),
                    stat = "identity", position = "dodge")+
           geom_text(aes(label=clusters_one_user),
                     position=position_dodge(width=0.9)
                     ,vjust=-0.3)+
           labs( x = "Test",
                 y = "Number Clusters",
                 title= "Number of clusters for each test with number of clusters with just one user"))

```


Louvain create small number of cluster in almost all of the cases. This clusters contain in general big amount of users and there are no clusters with just one user for Louvain. In contrast, Infomap generated for all of the tests clusters with just one user. In the case of Label propagation just for the first test it generated one cluster with just one user and for the third test, the number of clusters generated was smaller than for Louvain.

```{r echo=FALSE, warning=FALSE}
time_algorithms <- result %>%
  filter(measure == "time")

ggplotly(ggplot(data=time_algorithms, aes(x=id_test))+
           geom_bar(aes(y=value,fill=algorithm), 
                    stat = "identity", position = "dodge")+
           labs( x = "Test",
                 y = "Execution time (s)",
                 title = "Execution time per algorithm"))
```


The execution time for the tests was very similar for Label Propagation and Louvain. Whereas the execution time for Infomap is higher and it tends to increase as we process more data. Even though it is possible to find these differences between the execution time, it is not a determinant factor to select the clustering algorithm.  

Considering these tests, we selected Louvain as the clustering algorithm for the rest of the analysis in our project. For the previous graph representation of the data, the following communities were created with Louvain algorithm. 

```{r cluster_example}
#create communities
lc <- cluster_louvain(g)

# Get nodes and edges for the creation of the visualization of the community
nodes <-do.call(rbind.data.frame, as.list(V(g)$name)) %>%
  mutate(group = membership(lc)) %>%
  rename("id" = 1) %>%
  mutate(labels = id)
#get edges from data
edges <- get.data.frame(g, what= c("edges") )

visNetwork(nodes, edges)%>%
  visClusteringByGroup(groups = unique(nodes$group), label="cluster: ")%>%
  visInteraction(navigationButtons = TRUE) %>%
  visPhysics(maxVelocity = 1,repulsion = list(centralGravity=-0.5,springLength=500)) 
```


To analyze the content of the comments for each cluster, we inspect some random clusters to find the most common words, which are displayed in the following wordclouds. For some clusters we can find that they use quite often the same words like "stocks" and "buy", but we also observed that the most frequent words in each cluster are different. In this example, for the selected clusters the most frequent words are "gme", "short", "fidelity" and "restricted".

```{r wordcloud, warning=FALSE}
par(mfrow=c(2,2),
    oma = c(0,0,0,0),
    mar = c(0,0,0,0))
get_word_cloud_community(raw_data,communities(lc),2)
get_word_cloud_community(raw_data,communities(lc),5)
get_word_cloud_community(raw_data,communities(lc),8)
get_word_cloud_community(raw_data,communities(lc),10)
```

```{r robinEvaluation}
library(robin)

#Finds if the community structure found is statistically significant by compare to a random curve
graphRandom <- random(graph=g)
Proc <- robinRobust(graph=g, graphRandom=graphRandom, method="louvain",
                    measure="vi",type="independent")
plotRobin(graph=g, model1=Proc$Mean, model2=Proc$MeanRandom,
          measure="vi", legend=c("Random", "Louvain"))
robinAUC(graph=g, model1=Proc$Mean, model2=Proc$MeanRandom)
#get Bayes Factor
BFLouvain <- robinGPTest(model1=Proc$Mean,model2=Proc$MeanRandom)
#return the fitted curves and the adjusted p-values
robinFDATest(graph=g, model1=Proc$Mean, model2=Proc$MeanRandom,
             measure="vi",legend=c("Louvain", "Null Model"))

#compare two algorithms in the same initial graph
#takes a lot of time Comparison Louvain and Infomap
# compare <- robinCompare(graph=g, method1="louvain",
#              method2="infomap", measure="vi", type="independent")
# 
# plotRobin(graph=g, model1=compare$Mean1, model2=compare$Mean2,
#           measure="vi", legend=c("Louvain", "Infomap"))
# robinAUC(graph=g, model1=compare$Mean1, model2=compare$Mean2)

#compare louvain with label propagation 
compareLouvainLP <- robinCompare(graph=g, method1="louvain",
             method2="labelProp", measure="vi", type="independent")
plotRobin(graph=g, model1=compareLouvainLP$Mean1, model2=compareLouvainLP$Mean2,
          measure="vi", legend=c("Louvain", "LabelPropagation"))
robinAUC(graph=g, model1=compareLouvainLP$Mean1, model2=compareLouvainLP$Mean2)

# plotRobin(graph=g, model1=compare$Mean2, model2=compareLouvainLP$Mean2,
#           measure="vi", legend=c("Infomap", "LabelPropagation"))

```



# Exploratory Data Analysis

Initially we explore how the data is distributed in both reddits by getting the number of comments and posts per day. In general, the number of comments per day oscillates between 5 comment and 1039. For the week between 20.03.2021 and 27.03.2021 between 1 and 4 comments were retrieved. This difference was generated by the API used in the extraction. 

```{r comments_subreddits, echo=FALSE }
comments_per_day_stocks <-getCommentsPerDay(databaseName,"stocks") %>%
  mutate(subreddit = "Stocks")
comments_per_day_wallstreetbets <-getCommentsPerDay(databaseName,"wallstreetbets") %>%
  mutate(subreddit = "Wallstreetbets") %>%
  rbind(.,comments_per_day_stocks) %>%
  rename(date='_id')%>%
  mutate(dayWeek = weekdays(as.Date(date)),
         cw = strftime(date, format = "%V"))

#change date format
comments_per_day_wallstreetbets$date <- ymd(comments_per_day_wallstreetbets$date)


ggplotly(ggplot(data=comments_per_day_wallstreetbets, aes(x=date)) +
           geom_line(aes(y=total, color=subreddit))+
           scale_x_date(date_labels = "%m-%Y")+
           labs(title="Comments per day",
                x = "Date",
                y = "Comments"))
```

In terms of number of different posts, we got between 1 and 10 post per day except for the the date 25.02.2021 in the wallstreetbets subreddit where we got 18 different posts.

```{r post_subreddits, echo=FALSE }
post_per_day_stocks <- getPostPerDay(databaseName,"stocks") %>%
  mutate(subreddit = "Stocks")
post_per_day_wallstreetbets <- getPostPerDay(databaseName,"wallstreetbets")%>%
  mutate(subreddit = "Wallstreetbets") %>%
  rbind(.,post_per_day_stocks) %>%
  rename(date=X_id.data) 
  

post_per_day_wallstreetbets$date <- ymd(post_per_day_wallstreetbets$date)

ggplotly(ggplot(data=post_per_day_wallstreetbets, aes(x=date)) +
           geom_line(aes(y=nr_post, color=subreddit))+
           scale_x_date(date_labels = "%m-%Y")+
           labs(title="Posts per day",
                x = "Date",
                y = "Comments"))

```

### User's interaction in subreddits


```{r}
active_users <-merge(getActiveUsers("stocks"),
                  getActiveUsers("wallstreetbets"),
                  by='_id', all = TRUE) %>%
  rename(comments_stocks = comments.x, comments_wsb = comments.y, name_user = '_id') %>%
  replace_na(list(comments_stocks = 0, comments_wsb = 0)) %>%
  filter(name_user!="[deleted]") %>%
  mutate(participation = comments_stocks - comments_wsb) %>%
  mutate(subreddit = ifelse(participation>0,"stocks","wallstreetbets")) %>%
  mutate(type_participation = ifelse(comments_stocks>0 & comments_wsb>0,"Both subreddits",
                         ifelse(comments_stocks>0,"Stocks","Wallstreetbets")))

# ggplot(active_users, aes(x=participation)) + geom_histogram(bins = 50)

summary_users <- active_users %>%
  group_by(type_participation) %>%
  summarise(prop=(n()/nrow(active_users))*100)

ggplot(summary_users, aes(x=factor(1), y=prop, fill=type_participation))+
  geom_bar(width = 1, stat = "identity") +
  coord_polar(theta="y") + 
  scale_fill_brewer(palette="Blues")+
  theme_minimal()+
  theme(panel.grid=element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        panel.border = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.title=element_text(size=14, face="bold"))+
  geom_text(aes(y = prop/7 + c(0, cumsum(prop)[-length(prop)]), 
                label = percent(prop/100)), size=5)
```




*What visualizations did you use to look at your data in different ways?*
*What are the different machine learning methods you considered?*
*Justify the decisions you made, and show any major changes to your ideas.*
*How did you reach these conclusions?*

# Final Analysis

*What did you learn about the data?*
*How did you answer the questions?*
*How can you justify your answers?*

# References
